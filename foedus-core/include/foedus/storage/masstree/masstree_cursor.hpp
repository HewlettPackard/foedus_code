/*
 * Copyright (c) 2014-2015, Hewlett-Packard Development Company, LP.
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 * Software Foundation; either version 2 of the License, or (at your option)
 * any later version.
 *
 * This program is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
 * more details. You should have received a copy of the GNU General Public
 * License along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
 *
 * HP designates this particular file as subject to the "Classpath" exception
 * as provided by HP in the LICENSE.txt file that accompanied this code.
 */
#ifndef FOEDUS_STORAGE_MASSTREE_MASSTREE_CURSOR_HPP_
#define FOEDUS_STORAGE_MASSTREE_MASSTREE_CURSOR_HPP_

#include <stdint.h>

#include <cstring>

#include "foedus/assert_nd.hpp"
#include "foedus/cxx11.hpp"
#include "foedus/engine.hpp"
#include "foedus/assorted/endianness.hpp"
#include "foedus/storage/fwd.hpp"
#include "foedus/storage/page.hpp"
#include "foedus/storage/storage_id.hpp"
#include "foedus/storage/masstree/fwd.hpp"
#include "foedus/storage/masstree/masstree_id.hpp"
#include "foedus/storage/masstree/masstree_storage.hpp"
#include "foedus/thread/fwd.hpp"
#include "foedus/xct/xct_id.hpp"

namespace foedus {
namespace storage {
namespace masstree {
/**
 * @brief Represents a cursor object for Masstree storage.
 * @ingroup MASSTREE
 * @details
 * @par Dynamic Memory
 * This cursor objects uses a few dymanically allocated memory for keys, route information,
 * etc. However, we of course can't tolerate new/delete during transaction.
 * We thus grab memory from the memory pool for snapshot pages (could be volatile page's, but
 * probably volatile memory pool is more limiting, so let's grab it from snapshot memory pool).
 * These memory are allocated via libnuma and using THP, so it's also faster.
 * The only drawback is that we consume them in a little bit generous way, but shouldn't be a
 * big issue.
 * All of them are returned to the pool in destructor.
 */
class MasstreeCursor CXX11_FINAL {
 public:
  struct Route {
    enum MovedPageSearchStatus {
      kNotMovedPage = 0,
      kMovedPageSearchedNeither = 1,
      kMovedPageSearchedOne = 2,
      kMovedPageSearchedBoth = 3,
    };
    MasstreePage* page_;
    /** version as of getting calculating order_. */
    PageVersionStatus stable_;
    /** index in ordered keys. in interior, same. */
    uint8_t index_;
    /** only for interior. */
    uint8_t index_mini_;
    /** same as stable_.get_key_count() */
    uint8_t key_count_;
    /** only for interior. */
    uint8_t key_count_mini_;

    /**
     * Upto which separator we are done. only for interior.
     * If forward search, we followed a pointer before this separator.
     * If backward search, we followed a pointer after this separator.
     * This is updated whenever we follow a pointer from this interior page,
     * and used when we have to re-find the separator. In Master-tree, a separator never
     * disappears from the page, so we can surely find this.
     */
    KeySlice latest_separator_;

    /** whether page_ is a snapshot page */
    bool    snapshot_;
    /**
     * This is set to true when the initial locate() didn't find a matching record,
     * hitting the page boundary. in that case, locate() stops there and open() invokes
     * next().
     */
    bool    locate_miss_in_page_;

    /** only when stable_ indicates that this page is a moved page */
    MovedPageSearchStatus moved_page_search_status_;

    /** only for border. order_[0] is the index of smallest record, [1] second smallest... */
    uint8_t order_[64];

    uint8_t get_cur_original_index() const ALWAYS_INLINE {
      return order_[index_];
    }
    uint8_t get_original_index(uint8_t index) const ALWAYS_INLINE {
      return order_[index];
    }
    bool    is_valid_record() const ALWAYS_INLINE {
      return index_ < key_count_;
    }
    void setup_order();
  };
  enum SearchType {
    kForwardInclusive = 0,
    kForwardExclusive,
    kBackwardInclusive,
    kBackwardExclusive,
  };
  enum KeyCompareResult {
    kCurKeySmaller,
    kCurKeyEquals,
    kCurKeyLarger,
    // the following two are only when cur key points to next layer. instead, no equals for layer
    kCurKeyBeingsWith,
    kCurKeyContains,
  };
  enum Constants {
    kMaxRecords = 64,
    kMaxRoutes = kPageSize / sizeof(Route),
    kKeyLengthExtremum = 0,
  };

  MasstreeCursor(MasstreeStorage storage, thread::Thread* context);

  thread::Thread*   get_context() { return context_; }
  MasstreeStorage&  get_storage() { return storage_; }
  bool              is_for_writes() const { return for_writes_; }
  bool              is_forward_cursor() const { return forward_cursor_; }

  ErrorCode   open(
    const char* begin_key = CXX11_NULLPTR,
    uint16_t begin_key_length = kKeyLengthExtremum,
    const char* end_key = CXX11_NULLPTR,
    uint16_t end_key_length = kKeyLengthExtremum,
    bool forward_cursor = true,
    bool for_writes = false,
    bool begin_inclusive = true,
    bool end_inclusive = false);

  ErrorCode   open_normalized(
    KeySlice begin_key,
    KeySlice end_key,
    bool forward_cursor = true,
    bool for_writes = false,
    bool begin_inclusive = true,
    bool end_inclusive = false) {
    ASSERT_ND((forward_cursor && end_key >= begin_key) ||
      (!forward_cursor && end_key <= begin_key));
    KeySlice begin_key_be = assorted::htobe<KeySlice>(begin_key);
    KeySlice end_key_be = assorted::htobe<KeySlice>(end_key);
    return open(
      reinterpret_cast<const char*>(&begin_key_be),
      sizeof(KeySlice),
      reinterpret_cast<const char*>(&end_key_be),
      sizeof(KeySlice),
      forward_cursor,
      for_writes,
      begin_inclusive,
      end_inclusive);
  }

  bool      is_valid_record() const ALWAYS_INLINE {
    return route_count_ > 0 && !reached_end_ && cur_route()->is_valid_record();
  }
  const char* get_key() const ALWAYS_INLINE {
    ASSERT_ND(is_valid_record());
    return cur_key_;
  }
  uint16_t  get_key_length() const ALWAYS_INLINE {
    ASSERT_ND(is_valid_record());
    return cur_key_length_;
  }
  const char* get_payload() const ALWAYS_INLINE {
    ASSERT_ND(is_valid_record());
    return cur_payload_;
  }
  uint16_t  get_payload_length() const ALWAYS_INLINE {
    ASSERT_ND(is_valid_record());
    return cur_payload_length_;
  }

  ErrorCode next();


  ErrorCode delete_record();

  ErrorCode overwrite_record(const void* payload, uint16_t payload_offset, uint16_t payload_count);
  template <typename PAYLOAD>
  ErrorCode overwrite_record_primitive(PAYLOAD payload, uint16_t payload_offset);

  template <typename PAYLOAD>
  ErrorCode increment_record(PAYLOAD* value, uint16_t payload_offset);

 private:
  Engine* const         engine_;
  MasstreeStorage       storage_;
  thread::Thread* const context_;
  xct::Xct* const current_xct_;

  bool        for_writes_;
  bool        forward_cursor_;
  bool        end_inclusive_;
  bool        reached_end_;

  /** If this value is zero, it means supremum. */
  uint16_t    end_key_length_;
  /** If this value is zero, it means supremum. */
  uint16_t    cur_key_length_;
  uint16_t    cur_payload_length_;
  /** If this value is zero, it means supremum. */
  uint16_t    search_key_length_;
  SearchType  search_type_;
  uint8_t     search_key_in_layer_extremum_;

  /** number of higher layer pages. the current border page is not included. so, might be 0. */
  uint16_t    route_count_;

  uint8_t     cur_key_in_layer_remaining_;
  KeySlice    cur_key_in_layer_slice_;
  xct::XctId  cur_key_observed_owner_id_;
  xct::LockableXctId* cur_key_owner_id_address;

  /** full big-endian key to terminate search. allocated in transaction's local work memory */
  char*       end_key_;

  /** full big-endian key of current record. allocated in transaction's local work memory */
  char*       cur_key_;

  /** full payload of current record. Directly points to address in current page */
  const char* cur_payload_;

  /** full big-endian key of current search. allocated in transaction's local work memory */
  char*       search_key_;

  /** stable version of teh current border page as of copying cur_page_. */
  PageVersionStatus cur_page_stable_;

  /** allocated in transaction's local work memory. */
  Route*      routes_;

  ErrorCode push_route(MasstreePage* page);
  void      fetch_cur_record(MasstreeBorderPage* page, uint8_t record);
  void      check_end_key();
  KeyCompareResult compare_cur_key_aginst_search_key(KeySlice slice, uint8_t layer) const;
  KeyCompareResult compare_cur_key_aginst_end_key() const;
  KeyCompareResult compare_cur_key(
    KeySlice slice,
    uint8_t layer,
    const char* full_key,
    uint16_t full_length) const;

  uint8_t       get_cur_index() const ALWAYS_INLINE {
    ASSERT_ND(is_valid_record());
    return cur_route()->get_cur_original_index();
  }
  MasstreePage* get_cur_page() const ALWAYS_INLINE {
    ASSERT_ND(is_valid_record());
    return cur_route()->page_;
  }

  Route*        cur_route() ALWAYS_INLINE {
    ASSERT_ND(route_count_ > 0);
    return routes_ + route_count_ - 1;
  }
  const Route*  cur_route() const ALWAYS_INLINE {
    ASSERT_ND(route_count_ > 0);
    return routes_ + route_count_ - 1;
  }

  template <typename T>
  ErrorCode allocate_if_not_exist(T** pointer);

  bool is_search_key_extremum() const ALWAYS_INLINE {
    return search_key_length_ == kKeyLengthExtremum;
  }
  bool is_end_key_supremum() const ALWAYS_INLINE {
    return end_key_length_ == kKeyLengthExtremum;
  }

  ErrorCode follow_foster(KeySlice slice);
  void extract_separators(KeySlice* separator_low, KeySlice* separator_high) const ALWAYS_INLINE;

  // locate_xxx is for initial search
  ErrorCode locate_layer(uint8_t layer);
  ErrorCode locate_border(KeySlice slice);
  ErrorCode locate_next_layer();
  ErrorCode locate_descend(KeySlice slice);

  // proceed_xxx is for next
  ErrorCode proceed_route();
  ErrorCode proceed_route_border();
  ErrorCode proceed_route_intermediate();
  ErrorCode proceed_pop();
  ErrorCode proceed_next_layer();
  ErrorCode proceed_deeper();
  ErrorCode proceed_deeper_border();
  ErrorCode proceed_deeper_intermediate();
  void      proceed_route_intermediate_rebase_separator();

  MasstreePage* resolve_volatile(VolatilePagePointer ptr) const;

  void assert_modify() const ALWAYS_INLINE {
#ifndef NDEBUG
    ASSERT_ND(for_writes_);
    ASSERT_ND(is_valid_record());
    ASSERT_ND(!cur_route()->snapshot_);
    ASSERT_ND(reinterpret_cast<Page*>(get_cur_page())->get_header().get_page_type()
      == kMasstreeBorderPageType);
#endif  // NDEBUG
  }
  void assert_route() const ALWAYS_INLINE {
#ifndef NDEBUG
    for (uint16_t i = 0; i + 1U < route_count_; ++i) {
      const Route* route = routes_ + i;
      ASSERT_ND(route->page_);
      if (route->stable_.is_moved()) {
        // then we don't use any information in this path
      } else if (reinterpret_cast<Page*>(route->page_)->get_header().get_page_type()
        == kMasstreeBorderPageType) {
        ASSERT_ND(route->index_ < kMaxRecords);
        ASSERT_ND(route->index_ < route->key_count_);
      } else {
        ASSERT_ND(route->index_ <= route->key_count_);
        ASSERT_ND(route->index_ <= kMaxIntermediateSeparators);
        ASSERT_ND(route->index_mini_ <= route->key_count_mini_);
        ASSERT_ND(route->index_mini_ <= kMaxIntermediateMiniSeparators);
      }
    }
#endif  // NDEBUG
  }
};

}  // namespace masstree
}  // namespace storage
}  // namespace foedus
#endif  // FOEDUS_STORAGE_MASSTREE_MASSTREE_CURSOR_HPP_
